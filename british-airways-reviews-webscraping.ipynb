{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c57028b9-aa43-44fb-a8ba-9502922f394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "Scraping complete. Data saved to british_airways_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the base URL and headers\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways/page/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "# Initialize an empty list to store review data\n",
    "reviews = []\n",
    "\n",
    "# Loop through the first 30 pages\n",
    "for page in range(1, 31):\n",
    "    url = f\"{base_url}{page}/?sortby=post_date%3ADesc&pagesize=100\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    review_articles = soup.find_all(\"article\", itemprop=\"review\")\n",
    "    \n",
    "    for article in review_articles:\n",
    "        try:\n",
    "            header = article.find(\"h2\", class_=\"text_header\").text.strip()\n",
    "            author = article.find(\"span\", itemprop=\"name\").text.strip()\n",
    "            date = article.find(\"time\", itemprop=\"datePublished\").text.strip()\n",
    "            place = article.find(\"h3\", class_=\"text_sub_header\").text.split(\"(\")[1].split(\")\")[0].strip()\n",
    "            content = article.find(\"div\", class_=\"text_content\").text.strip()\n",
    "            rating = article.find(\"span\", itemprop=\"ratingValue\").text.strip()\n",
    "            trip_verified = \"Trip Verified\" in content\n",
    "            \n",
    "            review_data = {\"header\": header, \"author\": author, \"date\": date, \"place\": place, \"content\": content, \"rating\": rating, \"trip_verified\": trip_verified}\n",
    "            \n",
    "            review_stats = article.find(\"div\", class_=\"review-stats\")\n",
    "            if review_stats:\n",
    "                for row in review_stats.find_all(\"tr\"):\n",
    "                    key = row.find(\"td\", class_=\"review-rating-header\").text.strip().lower().replace(\" \", \"_\")\n",
    "                    value = row.find(\"td\", class_=\"review-value\")\n",
    "                    if value:\n",
    "                        review_data[key] = value.text.strip()\n",
    "                    else:\n",
    "                        stars = len(row.find_all(\"span\", class_=\"star fill\"))\n",
    "                        review_data[key] = stars\n",
    "            \n",
    "            reviews.append(review_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing a review: {e}\")\n",
    "    \n",
    "    print(f\"Scraped page {page}\")\n",
    "    time.sleep(2)  # Sleep to avoid getting blocked\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(reviews)\n",
    "#df.to_csv(\"british_airways_reviews.csv\", index=False)\n",
    "df.to_csv(r'C:\\Users\\eddsw\\OneDrive\\Desktop\\PORTFOLIO\\Web scraping\\british_airways_reviews.csv', index = False) \n",
    "print(\"Scraping complete. Data saved to british_airways_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e45346-5844-4c9e-9d2a-6e04524afc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
